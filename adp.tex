\section{Curse of Dimentionality}
\emph{Curse-of-Dimensionality} is a term used to denote the fact that the number of states grows exponentially in the number of state variables. Most MDPs arising in practical applications suffer from the curse, i.e., have large number of states and it is difficult to compute $J^*/J_u\in \R^n$ exactly in such scenarios. Approximate dynamic programming (ADP) \cite{lspi,lspe,ALP,wang2014approximate} methods compute an approximate value function $\tilde{J}$ instead of $J^*/J_u$. In order to tackle the curse, ADP methods resort to dimensionality reduction by parameterizing the value function and/or the policy. Value-function based ADP methods form an important subclass of ADP methods whose brief overview is given below.
\section{Value Function Based Approximate Dynamic Programming}
In the value-function based ADP schemes a parameterized family is chosen and the approximate value-function $\tilde{J}$ is picked from the chosen parameterized class.
Once the approximate value function $\tilde{J}$ is computed, a sub-optimal policy $\tilde{u}$ can be obtained as the one-step greedy policy with respect to $\tilde{J}$ by making use of \eqref{subpol}.
The following lemma characterizes the degree of sub-optimality of the greedy policy $\tilde{u}$.
\begin{lemma}\label{subopt}
Let $\tilde{J}$ be the approximate value function and $\tilde{u}$ be as in \eqref{subpol}, then 
\begin{align}
||J^*-J_{\tilde{u}}||_\infty \leq \frac{2}{1-\alpha}||J^*-\tilde{J}||_\infty.
\end{align}
\end{lemma}
\begin{proof}
We know that
\begin{align}
\label{top} (T_{\tilde{u}}) \tilde{J}(s) 	&= g(s)+\alpha \sum_{s'} p_{\tilde{u}(s)} (s,s') \tilde{J}(s'),\\
\label{bot} J_{\tilde{u}}(s)		&=g(s)+\alpha \sum_{s'} p_{\tilde{u}(s)}(s,s')J_{\tilde{u}}(s').
\end{align}
Hence we can write by subtracting \eqref{top} from \eqref{bot} 
\begin{align}
J_{\tilde{u}}-\tilde{J}&=T_{\tilde{u}}\tilde{J}-\tilde{J} +\alpha P_{\tilde{u}}(J_{\tilde{u}}-\tilde{J}),\mb\text{or},\nn\\
J_{\tilde{u}}-\tilde{J}&=(I-\alpha P_{\tilde{u}})^{-1}(T_{\tilde{u}}\tilde{J}-\tilde{J}),\mb \text{hence}\nn\\
||J_{\tilde{u}}-\tilde{J}||_\infty&\leq \frac{1}{1-\alpha}||T_{\tilde{u}}\tilde{J}-\tilde{J}||_\infty.\nn
\end{align}
We know from \eqref{subpol} that $T_{\tilde{u}}\tilde{J}=T\tilde{J}$. Also from the fact that $J^*=TJ^*$ and the contraction property of $T$, we know $||T\tilde{J}-J^*||_\infty\leq \alpha ||\tilde{J}-J^*||_\infty$ and $||T_{\tilde{u}}\tilde{J}-\tilde{J}||_\infty\leq (1+\alpha)||\tilde{J}-J^*||_\infty$. Hence we have
\begin{align}
||J_{\tilde{u}}-J^*||_\infty&=||J_{\tilde{u}}-J^*+\tilde{J}-\tilde{J}||_\infty\nn\\
&\leq  ||J_{\tilde{u}}-\tilde{J}||_\infty+||\tilde{J}-J^*||_\infty\nn\\
&\leq \frac{1}{1-\alpha}||T_{\tilde{u}}\tilde{J}-\tilde{J}||_\infty +||J^*-\tilde{J}||_\infty\nn\\
&\leq \frac{1+\alpha}{1-\alpha}||J^*-\tilde{J}||_\infty +||J^*-\tilde{J}||_\infty\nn\\
&\leq \frac{2}{1-\alpha}||J^*-\tilde{J}||_\infty\nn.
\end{align}
\end{proof}
The quality of any ADP method depends on the approximation guarantees it offers for the quantities $||J^*-\tilde{J}||$ and $||J^*-J_{\tu}||$, where $||\cdot||$ is an appropriate norm. The term  $||J^*-\tilde{J}||$ denotes the error in prediction and $||J^*-J_{\tu}||$ denotes the loss in performance resulting from the sub-optimal policy $\tu$ with respect to the optimal policy $u^*$. Of the two error terms, $||J^*-J_{\tu}||$ is more important because ultimately we are interested in coming up with a useful policy. In the context of ADP methods, the control and prediction problems are said to be addressed when the error terms $||J^*-\tilde{J}||$ and $||J^*-J_{\tu}||$ are bounded by ``small'' constants.\\
\section{Linear Function Approximation}
The most widely used parameterized class to approximate the value-function is the linear function approximator (LFA). Under LFA, the value-function is approximated as $\tilde{J}=\Phi r^*$, with $\Phi=[\phi_1|\ldots|\phi_k]$ being an $n\times k$ feature matrix and $r^*$ is the parameter to be learnt.\\
%ADP methods vary in the way they compute the $\tj$ and $\tu$, and in what follows we present a short overview of some of the chief ADP methods while discussing their successes and shortcomings.
There are two important approaches to value function approximation. Both the approaches start out with a basic solution method and appropriately introduce function approximation in it. The two approaches are:
\begin{enumerate}
\item The Projected Bellman Equation (PBE) which combines the BE and the linear least squares projection operator to project high dimensional quantities onto the subspace of the LFA.
\item The approximate linear programming formulation which introduces the LFA in the linear programming formulation. 
\end{enumerate}

