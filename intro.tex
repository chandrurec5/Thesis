\chapter{Introduction}
\section{Optimal Sequential Decision Making Under uncertainity}
What does this thesis achieve?
\section{Markov Decision Process}
%\subsection{Examples}
\section {Dynamic Programming}
%\subsection{Problems of Control and Prediction}
\section{Practical Issues}
%\subsection{Curse of Dimensionality}
%\subsection{Lack of Model Information}
\section{Approximate Dynamic Programming}
%\subsection{Key Ingredients}
%\subsection{Performance Guarantees}
%\subsection{Open Questions}
\section{Reinforcement Learning}
%\subsection{Stochastic Approximation}
%\subsection{Open Questions}
\section{Contributions and Organization}
\section{Markov Decision Processes}
Markov Decision Process (MDP) is an useful framework to cast problems involving optimal sequential decision making under uncertainity. Such problems often occur in science, engineering and economics, with planning by autonomous agents, control of large queuing systems and inventory control being some specific examples. Informally, the decision maker/planner wishes to maximize its objective, and in order to achieve this end, needs to perform an appropriate \emph{action/control} at each decision epoch based on the \emph{state} of the underlying \emph{system}. 
%Informally, at each decision epoch, the decision maker/planner maker finds itself in a particular \emph{state} and has to perform an \emph{action} so as to maximize its objective. 
The sequence of actions/control is also known as \emph{policy} and the planner is interested in computing the \emph{optimal} policy. In most cases, in order to compute the optimal policy, the planner also needs to \emph{predict} the value of each state of any given control. Thus, with any given MDP, there are two natural sub-problems namely the problem of prediction and the problem of control.\par
Dynamic Programming (DP) is a general principle to solve MDPs, the core of which constitutes the \emph{Bellman} equation (BE). The Bellman equation relates the optimal policy to its corresponding value known as the optimal \emph{value function}, i.e., it relates the prediction and control problems of a given MDP. A host of analytical and iterative methods exists to solve the Bellman equation for MDPs. These methods are known as exact dynamic programming methods since they spell out the optimal control and its value function exactly.\par
The three important exact DP methods are 
\begin{itemize}
\item Value Iteration.
\item Policy Iteration.
\item Linear Programming.
\end{itemize}
Of these methods, value iteration and linear programming are termed as value function based DP methods since they compute the optimal value function first and the optimal policy is obtained by substituting it in the Bellman Equation. On the other hand, policy iteration predicts the value of a policy at each iterative step and then improves it to yeild a better policy in the next step eventually converging to the optimal policy. It is important to emphasize that these exact DP methods solve both the control and prediction problems.\par
\section{Practical Issues}
MDPs arising in practical problems are faced with two important issues. First issue is due to the \emph{curse-of-dimensionality} (or simply the \emph{curse}), a term which signifies the fact that the number of states of the MDP grows exponentially in the number of state variables. As the number of state variables increase, it becomes difficult to solve the MDP employing exact DP methods due to the computational overhead involved. Moreover, it also difficult to store and retrieve the exact value function and policy when the number of states are large.\par
The second issue is the lack of model information, wherein, the the model parameters of the MDP are not available explicitly. However, the the underlying system can be simulator or sample trajectories via direct interaction with the system. This scenario is known as the \emph{reinforcement learning} setting since the model parameters have to be \emph{learned} by using the feedback obtained via direct interaction with the environment.\par
A wide range of methods/algorithms exist in literature to address the issues of the curse and the lack of model information. In particular, the methods that tackle the curse are broadly termed as  approximate dynamic programming  methods and methods that handle the case of lack of model information are called reinforcement learning algorithms. 
\section{Approximate Dynamic Programming}
Approximate Dynamic Programming (ADP) is the name given to a class of approximate solution methods that tackle the curse. 
A major bottleneck the ADP methods face in the case of MDPs having a large number of states is that the value corresponding to each and every state cannot be stored, retrieved and computed. Most often ADP methods adopt an approximation architecture that enables compact representation of the value function in higher dimension so as to ease the storage and computation. Once the approximation architecture is fixed it is also important to devise a scheme that will choose the right candidate function that approximates the value function well. Thus central to any ADP method are
\begin{enumerate}
\item Approximation architecture.
\item Scheme to compute the right function within the chosen approximation architecture.
\end{enumerate}
In most cases, ADP methods are obtained by combining exact DP methods with a given approximation architecture. A given ADP scheme can be said to belong to either of two approaches based on the way the prediction and the control problems are addressed. The two distinct approaches are:
\begin{enumerate}
\item The \emph{value function} based approach, wherein a direct approximation to the optimal value function is obtained and a \emph{greedy/sub-optimal} policy is computed by substituing the approximate value function in the Bellman equation.
\item The \emph{approximate policy iteration} based approach, wherein, the critic evaluates the current policy, i.e., computes an approximation of the value function of the current policy. The actor on the other hand, makes use of the approximate value function to imporve the current policy.
\end{enumerate}
 Further, the performance of a given ADP method can be quantified by the following metrics namely
\begin{itemize}
\item Prediction Error, i.e., the difference between the exact value function and the approximate value function.
\item Control Error, i.e., the loss in performance due to the sub-optimal policy as compared to the optimal policy.
\end{itemize}
It is a known result that if the prediction error is bounded in the $\max$-norm (or $L_\infty$-norm), the control error can be bounded as well. The necessacity of $L_\infty$-norm  arises due to the fact that the Bellman operator is only a contraction map in the $L_\infty$ norm.\par
An ADP method is said to address both the prediction and control problems if it can offer guaranteed performance levels measured in terms of the aforementioned metrics. Given an ADP method, it a major theoretical challenge to come up with an analytical expression to bound the aforementioned performance metrics and it is in particlar difficult to bound the control error in many ADP schemes.\par
\section{Linear Function Approximation}
The first step in any ADP method is the choice of approximation architecture. In particular, a compact way of representing an approximation for the value function is necessary in most cases. A paramtererized function class is an useful approximation architecture since every function can be specified by the parameter it is enough to store only the parameters. Computing the right function that best approximates the exact value function then boils down to computations involving only the parameters and the curse can be tackled by choosing the number of parameters to be much lesser than the number of states. The most widely used parameterized class is the linear function approximator (LFA). Under a LFA, each function is written as a linear combination of pre-selected \emph{basis} functions. The exact value function is then approximated by computing/learning the weights of the various basis functions in the linear combination.\par
The method used to compute the right weights of the linear combination affects the quality of the approximation. The projected Bellman equation and the approximate linear programming are two different ways of computing the weights. These two differing approaches have their advantages/disadvantages and provide interesting research problems.
\subsection{Projected Bellman Equation}
Once the basis functions of the LFA have been fixed, the right candidate function from the LFA has to be picked as the approximate value function. A candidate for the approximate value function could be that function which is at the least distance from the exact value function. Minimizing the distance between the linear combination of basis functions and the exact value function is similar to the idea of conventional \emph{linear regression} wherein, the target function is \emph{projected} onto the linear sub-space spanned by the basis functions. However, the idea of linear regression cannot be applied in a straightforward manner to compute the approximate value function because the exact value function (which is the target function in this case) is not known.\par
The Projected Bellman Equation (PBE) combines the idea of linear least squares projection and the Bellman equation. The central idea underlying the PBE is to find a fixed point in the linear sub-space spanned by the basis functions. However, existence of such a fixed point can be ensured only under a restricted setting. In particular, the Bellam operator is \emph{non-linear} and is a contraction map in the $\max$-norm ($L_\infty$-norm). On the other hand, the linear least squares projection operator is non-expansive only in the $L_2$-norm. Hence the Bellman operator cannot be combined as such with the least squares projection operator. Neverthless, this issue can be side stepped by considering the Bellman operator restricted to a given policy which can be shown to be a contraction map in a generalized $L_2$-norm. The Bellman operator restricted to a given policy can be combined with the linear least squares projection operator to find a fixed point. However, such a fixed point can approximate only the value function of the particular policy (with respect to which the Bellman operator has been restricted) and not the optimal value function. As a consequence, the PBE can only be used for approximate policy evaluation, i.e., it can be used to compute an approximation to the value function of a given policy. This means that the PBE based methods fall into the category of approximate policy iteration approach, i.e., the approximate policy evaluation should be followed by a policy improvement step. However, there is a `norm-mismatch' between the projection operator that minimizes the error in $L_2$-norm and the $\max/L_\infty$-norm required to guarantee policy improvement. As a result, the sub-optimal policy obtained from the approximate value function computed by the PBE is not guaranteed to be an improvement.\par
The major disadvantage of the PBE based methods is that they do not address the control problem completely. In fact, there are simple MDPs for which the PBE based solution methods are known to produce a sequence of policies that oscillate within a set of bad policies. This phenomenon is called as \emph{policy-chattering} and is a direct consequence of the norm mismatch.\par
\textbf{\underline{Problem~$1$: Projected Bellman Equation in the $\minp$ linear basis}}\\
It is known that \cite{BertB} the issue of norm-mismatch can be alleviated if the projection operator preserves \emph{monotonicity}, i.e., if given two functions with one of them greater (component-wise) that the other, then the same holds for their projections as well. It is also known that the projection operator arising in the $\minp$ linear algebra preserves this monotonicity property. The first part of the thesis deals with developing convergent ADP methods based on $\minp$ linear algebra. In particular, the monotonicity property helps in
\begin{itemize}
\item Building convergent value function based ADP methods. Since the optimal value function is approximated directly, the issue of policy-chattering is absent.
\item Providing performance guarantees for the greedy/sub-optimal policy. This is possible since there is no issue of `norm-mismatch'.
\end{itemize}
\subsection{Approximate Linear Programming}
The approximate linear programming (ALP) formulation is obtained by introducing linear function approximation in the linear programming formulation (LP) of the given MDP. In contrast to the PBE, the ALP does not rely on the linear least squares projection operator, but instead, computes the approximate value function by optimizing a linear objective over a set of linear inequality constraints. In particular, the ALP restricts its search to a space of linear functions that upper bound the optimal value function. The ALP is a value function method as it computes an approximation to the optimal value function. Since, the corresponding greedy policy can be derived in a straightforward manner, the ALP does not suffer from issues of policy-chattering. Further, the ALP also offers good performance guarantees for both the prediction as well as the control problems, and thus addresses both the problems.\par
A significant shortcoming of the ALP formulation is that the number of constraints is of the order of the state space. Most MDPs arising in practice have a large number of constraints and hence it is always not possible to solve the ALP with all the constraints. However, there are some special cases of factored MDPs with factored value function representations that enable elimination of variables to come up with a tractable number of constraints. A general approach is to handle the large number of constraints is constraint sampling, wherein a Reduced Linear Program (RLP) is formulated by sampling fewer number of constraints from the orginial constraints of the ALP. Though the RLP is known to perform well in experiments, the theoretical analysis is available only for a specific RLP formulated under idealized settings.\par
\textbf{\underline{Problem~$2$: Framework to analyse constraint reduction in ALP:}}\\
The second part of the thesis deals with developing a novel theoretical framework to analyse constraint reduction/approximation in the ALP. The framework is built around studying a Generalized Reduced Linear Program (GRLP) whose constraints are obtained as positive linear combinations of the original constraints of the ALP. The salient features of the framework are
\begin{enumerate}
\item The analysis is based on two novel contraction maps and the error bounds are provided in a modified $L_\infty$-norm. Both the prediction and control error are bounded, thus making the GRLP a complete ADP scheme.
\item Justification of linear function approximation of the Lagrange multipliers associated with the constraints of the ALP. This is a desirable outcome, since both the primal and dual variables have linear function approximation.
\end{enumerate}
\section{Reinforcement Learning}
Reinforcement Learning (RL) algorithms can be viewed as `on-line'/sample trajectory based solution methods for solving MDPs. In the case of MDPs with a large number of states, RL algorithms are obtained as on-line versions of ADP methods. In order to handle the noise in the sample trajectory RL algorithms make use of stochastic approximation (SA). Typically, RL algorithm employing stochastic approximation are iterative schemes which take a small \emph{step} towards the desired value at each iteration. By making the right choice of the \emph{step-size} schedule effect of noise can be nullified and convergence to the desired value can be guaranteed.\par
Actor-Critic algorithms form an important sub-class of RL algorithms, wherein, the critic is responsible for policy evalutaion and the actor is responsible for policy improvement. In order to tackle the curse, the actor-critic schemes parameterize the policy and the value function. In an actor-critic algorithm, the critic forms the inner-loop and the actor constitutes the outer-loop. This is due to the fact that the actor has to needs the critic to evaluate a given policy before the actor updates the policy parameters. This effect of having two seperate loops can instead by achieved in practice by adopting different \emph{step-size} schedules for the actor and the critic. Specifically, the step-sizes used by the actor updates have to be much smaller than those used in the critic updates.\par
Stochastic approximation schemes that use different step-size schedules for different sets of iterates are known as multi timescale stochastic approximation schemes. The conditions under which the iterates of a multi timescale SA schemes converge to the desired value have been studied in literature. One of the conditions required to ensure the convergence of the iterates of a multi timescale SA scheme is that the iterates need to be stable, i.e., they should be bounded. However, the conditions that \emph{imply} the stability of the iterates in a multi timescale SA scheme have not been understood.\\
\textbf{\underline{Problem~$3$: Stability Criterion for Two Timescale Stochastic Approximation Schemes:}}\\
The third part of the thesis deals with providing conditions under which the stability of iterates in a two timescale stochastic approximation scheme follows. Salient features of the contribution are as follows:
\begin{enumerate}
\item The analysis is based on the ODE approach to stochastic approximation.
\item Stability of iterates of important actor-critic algorithms follow for the result.
\end{enumerate}
