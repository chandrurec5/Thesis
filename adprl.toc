\contentsline {chapter}{\numberline {1}Introduction}{1}
\contentsline {section}{\numberline {1.1}Markov Decision Process}{1}
\contentsline {section}{\numberline {1.2}Practical Issues}{2}
\contentsline {section}{\numberline {1.3}Approximate Dynamic Programming}{3}
\contentsline {section}{\numberline {1.4}Linear Function Approximation}{4}
\contentsline {section}{\numberline {1.5}Reinforcement Learning}{7}
\contentsline {section}{\numberline {1.6}Contributions and Organization}{8}
\contentsline {chapter}{\numberline {2}Markov Decision Processes (MDPs)}{9}
\contentsline {section}{\numberline {2.1}Definition}{9}
\contentsline {section}{\numberline {2.2}Inifinte Horizon Discounted Reward}{10}
\contentsline {section}{\numberline {2.3}Bellman Equation}{11}
\contentsline {section}{\numberline {2.4}Bellman Operator}{12}
\contentsline {section}{\numberline {2.5}Basic Solution Methods}{13}
\contentsline {section}{\numberline {2.6}Curse of Dimentionality}{15}
\contentsline {section}{\numberline {2.7}Value Function Based Approximate Dynamic Programming}{15}
\contentsline {section}{\numberline {2.8}Linear Function Approximation}{17}
\contentsline {section}{\numberline {2.9}Control and Prediction}{17}
\contentsline {chapter}{\numberline {3}Approximate Dynamic Programming in $(\qopname \relax m{min},+)$ linear basis}{18}
\contentsline {section}{\numberline {3.1}Projected Bellman Equation}{19}
\contentsline {section}{\numberline {3.2}Monotone Maps}{20}
\contentsline {section}{\numberline {3.3}$(\qopname \relax m{min},+)$ linear functions}{20}
\contentsline {section}{\numberline {3.4}Fenchel Dual and Projection on Subsemimodules}{22}
\contentsline {section}{\numberline {3.5} Approximate $Q$ Iteration}{24}
\contentsline {section}{\numberline {3.6}Variational Approximate Q Iteration}{25}
\contentsline {section}{\numberline {3.7}Error Analysis}{26}
\contentsline {subsection}{\numberline {3.7.1}Experiment in Grid World}{27}
\contentsline {subsection}{\numberline {3.7.2}The Mountain Car Experiment}{30}
\contentsline {chapter}{\numberline {4}Approximate Linear Program}{34}
\contentsline {section}{\numberline {4.1}Approximate Linear Programming}{34}
\contentsline {section}{\numberline {4.2}Lyanpunov Functions}{37}
\contentsline {section}{\numberline {4.3}Constraint sampling}{37}
\contentsline {chapter}{\numberline {5}Generalized Reduced Linear Program}{40}
\contentsline {section}{\numberline {5.1}Generalized Reduced Linear Program}{40}
\contentsline {section}{\numberline {5.2}Least Upper Bound Projection}{42}
\contentsline {section}{\numberline {5.3}Approximate Least Upper Bound Projection}{46}
\contentsline {section}{\numberline {5.4}A simple bound}{48}
\contentsline {section}{\numberline {5.5}Improved Bounds}{50}
\contentsline {section}{\numberline {5.6}Discussion}{54}
\contentsline {subsection}{\numberline {5.6.1}On Error Terms}{54}
\contentsline {subsection}{\numberline {5.6.2}On Constraint Reduction and Approximation}{55}
\contentsline {subsection}{\numberline {5.6.3}Numerical Illustration}{57}
\contentsline {subsection}{\numberline {5.6.4}Reinforcement Learning}{59}
\contentsline {section}{\numberline {5.7}Conclusion}{60}
\contentsline {chapter}{\numberline {6}Stochastic Approximation and Reinforcement Learning}{61}
\contentsline {section}{\numberline {6.1}Lack of Model Information}{61}
\contentsline {section}{\numberline {6.2}Reinforcement Learning}{61}
\contentsline {section}{\numberline {6.3}Two Timescale Stochastic Approximation}{61}
\contentsline {section}{\numberline {6.4}Actor Critic Algorithms}{61}
\contentsline {chapter}{\numberline {7}Stability of Iterates in Two Timescale Stochastic Approximation}{62}
\contentsline {section}{\numberline {7.1}Introduction}{62}
\contentsline {section}{\numberline {7.2}Results for the Scaled ODE}{66}
\contentsline {section}{\numberline {7.3}Stability of Two Timescale Stochastic Approximation Algorithms}{72}
\contentsline {subsection}{\numberline {7.3.1}Faster Timescale Results}{73}
\contentsline {subsection}{\numberline {7.3.2}Slower Timescale Analysis}{78}
\contentsline {section}{\numberline {7.4}An Application in Reinforcement Learning}{82}
\contentsline {section}{\numberline {7.5}Conclusions}{89}
\contentsline {section}{\numberline {7.6}Appendix}{90}
\contentsline {subsection}{\numberline {7.6.1}Gronwall Inequalities}{90}
